{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "def normalize_document(doc):\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]','',doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30103\n",
      "['1', ':', '6', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'a', 'firmament', 'in', 'the', 'midst', 'of', 'the', 'waters', ',', 'and', 'let', 'it', 'divide', 'the', 'waters', 'from', 'the', 'waters', '.']\n",
      "1 : 6 And God said , Let there be a firmament in the midst of the waters , and let it divide the waters from the waters .\n"
     ]
    }
   ],
   "source": [
    "bible  =  gutenberg.sents('bible-kjv.txt')\n",
    "print(len(bible))\n",
    "print(bible[10])\n",
    "print(' '.join(bible[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_term = punctuation + '0123456789'\n",
    "norm_bible = [[word.lower() for word in sent if word not in remove_term] for sent in bible]\n",
    "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
    "norm_bible = filter(None,normalize_corpus(norm_bible))\n",
    "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split())>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Line: 30103\n",
      "Sample Line: ['1', ':', '6', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'a', 'firmament', 'in', 'the', 'midst', 'of', 'the', 'waters', ',', 'and', 'let', 'it', 'divide', 'the', 'waters', 'from', 'the', 'waters', '.']\n",
      "Normalized: god said let firmament midst waters let divide waters waters\n"
     ]
    }
   ],
   "source": [
    "print('Total Line:', len(bible))\n",
    "print('Sample Line:', bible[10])\n",
    "print('Normalized:', norm_bible[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBoW\n",
    "- vocabulary\n",
    "- pairwose context words\n",
    "- CBOW\n",
    "- Train\n",
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples [('shall', 1), ('unto', 2), ('lord', 3), ('thou', 4), ('thy', 5), ('god', 6), ('ye', 7), ('said', 8), ('thee', 9), ('upon', 10)]\n",
      "Vocabulary Size 12425\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "tokenizer =  text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id= tokenizer.word_index\n",
    "word2id['PAD'] = 0\n",
    "\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)]for doc in norm_bible]\n",
    "\n",
    "print('Samples', list(word2id.items())[:10])\n",
    "print('Vocabulary Size', len(word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_pairs(corpus,window_size,vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words =[]\n",
    "            label_word = []\n",
    "            start = index - window_size\n",
    "            end = index + window_size+1\n",
    "            \n",
    "            context_words.append([words[i] for i in range(start, end) if 0<=i<sentence_length and i!=index])\n",
    "            label_word.append(word)\n",
    "            \n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = to_categorical(label_word, vocab_size)\n",
    "            yield (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['old', 'testament', 'james', 'bible'] -> Target (Y): king\n",
      "Context (X): ['first', 'book', 'called', 'genesis'] -> Target (Y): moses\n",
      "Context (X): ['beginning', 'god', 'heaven', 'earth'] -> Target (Y): created\n",
      "Context (X): ['earth', 'without', 'void', 'darkness'] -> Target (Y): form\n",
      "Context (X): ['without', 'form', 'darkness', 'upon'] -> Target (Y): void\n",
      "Context (X): ['form', 'void', 'upon', 'face'] -> Target (Y): darkness\n",
      "Context (X): ['void', 'darkness', 'face', 'deep'] -> Target (Y): upon\n",
      "Context (X): ['spirit', 'god', 'upon', 'face'] -> Target (Y): moved\n",
      "Context (X): ['god', 'moved', 'face', 'waters'] -> Target (Y): upon\n",
      "Context (X): ['god', 'said', 'light', 'light'] -> Target (Y): let\n",
      "Context (X): ['god', 'saw', 'good', 'god'] -> Target (Y): light\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x, y in word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object word_pairs at 0x00000206BFD7B430>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pairs(wids,window_size,vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CBOW Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            1242500   \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12425)             1254925   \n",
      "=================================================================\n",
      "Total params: 2,497,425\n",
      "Trainable params: 2,497,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Lambda, Dense\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x,axis=1), output_shape=(embed_size,1)))\n",
    "cbow.add(Dense(vocab_size,activation='softmax'))\n",
    "\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(cbow.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1,51):\n",
    "    loss = 0.0\n",
    "    i=0\n",
    "    for x,y in word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        loss+= cbow.train_on_batch(x,y)\n",
    "        i+=1\n",
    "        if i%100==0:\n",
    "            print('Processed {}'.format(i))\n",
    "    print('Epoch: ', epoch, '\\t', loss)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12424, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>unto</th>\n",
       "      <td>0.005532</td>\n",
       "      <td>-0.028494</td>\n",
       "      <td>0.027624</td>\n",
       "      <td>-0.046013</td>\n",
       "      <td>0.034911</td>\n",
       "      <td>-0.044818</td>\n",
       "      <td>0.037544</td>\n",
       "      <td>0.041854</td>\n",
       "      <td>-0.061618</td>\n",
       "      <td>-0.011193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027403</td>\n",
       "      <td>0.006258</td>\n",
       "      <td>0.065401</td>\n",
       "      <td>-0.020276</td>\n",
       "      <td>-0.024361</td>\n",
       "      <td>-0.003698</td>\n",
       "      <td>-0.015867</td>\n",
       "      <td>0.130519</td>\n",
       "      <td>-0.062761</td>\n",
       "      <td>0.081934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lord</th>\n",
       "      <td>-0.012903</td>\n",
       "      <td>-0.008829</td>\n",
       "      <td>-0.113023</td>\n",
       "      <td>0.086098</td>\n",
       "      <td>-0.054459</td>\n",
       "      <td>-0.047707</td>\n",
       "      <td>-0.016049</td>\n",
       "      <td>0.023504</td>\n",
       "      <td>0.036188</td>\n",
       "      <td>0.109216</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023334</td>\n",
       "      <td>0.036802</td>\n",
       "      <td>0.035012</td>\n",
       "      <td>0.030573</td>\n",
       "      <td>0.050417</td>\n",
       "      <td>0.018698</td>\n",
       "      <td>-0.022541</td>\n",
       "      <td>0.123939</td>\n",
       "      <td>0.030760</td>\n",
       "      <td>0.069052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thou</th>\n",
       "      <td>-0.078650</td>\n",
       "      <td>0.047485</td>\n",
       "      <td>-0.047135</td>\n",
       "      <td>0.071397</td>\n",
       "      <td>-0.081151</td>\n",
       "      <td>-0.102222</td>\n",
       "      <td>0.087451</td>\n",
       "      <td>-0.062817</td>\n",
       "      <td>0.099307</td>\n",
       "      <td>0.132839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086158</td>\n",
       "      <td>0.017178</td>\n",
       "      <td>-0.006755</td>\n",
       "      <td>0.014293</td>\n",
       "      <td>0.030317</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.057206</td>\n",
       "      <td>-0.052498</td>\n",
       "      <td>0.022344</td>\n",
       "      <td>-0.071545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thy</th>\n",
       "      <td>-0.090057</td>\n",
       "      <td>0.025937</td>\n",
       "      <td>-0.124368</td>\n",
       "      <td>0.030982</td>\n",
       "      <td>0.029679</td>\n",
       "      <td>-0.147876</td>\n",
       "      <td>0.075110</td>\n",
       "      <td>-0.033713</td>\n",
       "      <td>0.064994</td>\n",
       "      <td>0.145510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020966</td>\n",
       "      <td>0.020684</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>-0.018096</td>\n",
       "      <td>0.018874</td>\n",
       "      <td>-0.070777</td>\n",
       "      <td>-0.042339</td>\n",
       "      <td>0.023420</td>\n",
       "      <td>-0.041602</td>\n",
       "      <td>0.064784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>god</th>\n",
       "      <td>-0.066040</td>\n",
       "      <td>0.006885</td>\n",
       "      <td>-0.074482</td>\n",
       "      <td>0.082840</td>\n",
       "      <td>-0.008219</td>\n",
       "      <td>-0.065309</td>\n",
       "      <td>0.027086</td>\n",
       "      <td>-0.012326</td>\n",
       "      <td>-0.011355</td>\n",
       "      <td>0.105838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>-0.009807</td>\n",
       "      <td>-0.031183</td>\n",
       "      <td>0.082439</td>\n",
       "      <td>-0.046060</td>\n",
       "      <td>0.041017</td>\n",
       "      <td>0.028875</td>\n",
       "      <td>0.085337</td>\n",
       "      <td>0.045473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "unto  0.005532 -0.028494  0.027624 -0.046013  0.034911 -0.044818  0.037544   \n",
       "lord -0.012903 -0.008829 -0.113023  0.086098 -0.054459 -0.047707 -0.016049   \n",
       "thou -0.078650  0.047485 -0.047135  0.071397 -0.081151 -0.102222  0.087451   \n",
       "thy  -0.090057  0.025937 -0.124368  0.030982  0.029679 -0.147876  0.075110   \n",
       "god  -0.066040  0.006885 -0.074482  0.082840 -0.008219 -0.065309  0.027086   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "unto  0.041854 -0.061618 -0.011193  ... -0.027403  0.006258  0.065401   \n",
       "lord  0.023504  0.036188  0.109216  ... -0.023334  0.036802  0.035012   \n",
       "thou -0.062817  0.099307  0.132839  ...  0.086158  0.017178 -0.006755   \n",
       "thy  -0.033713  0.064994  0.145510  ... -0.020966  0.020684  0.001814   \n",
       "god  -0.012326 -0.011355  0.105838  ...  0.020500  0.002825 -0.009807   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "unto -0.020276 -0.024361 -0.003698 -0.015867  0.130519 -0.062761  0.081934  \n",
       "lord  0.030573  0.050417  0.018698 -0.022541  0.123939  0.030760  0.069052  \n",
       "thou  0.014293  0.030317  0.002620  0.057206 -0.052498  0.022344 -0.071545  \n",
       "thy  -0.018096  0.018874 -0.070777 -0.042339  0.023420 -0.041602  0.064784  \n",
       "god  -0.031183  0.082439 -0.046060  0.041017  0.028875  0.085337  0.045473  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12424, 12424)\n"
     ]
    }
   ],
   "source": [
    "# find the similar of words or cluster of word\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'god': ['tents', 'gods', 'bird', 'children', 'sort'],\n",
       " 'lord': ['return', 'adam', 'therein', 'high', 'james'],\n",
       " 'thou': ['saying', 'began', 'unto', 'canaan', 'male'],\n",
       " 'thy': ['went', 'canaan', 'nakedness', 'two', 'sort'],\n",
       " 'john': ['imagineth', 'ether', 'benhanan', 'priest', 'riding'],\n",
       " 'gospel': ['undersetters', 'herewith', 'foameth', 'presented', 'beryl'],\n",
       " 'moses': ['moving', 'dwelt', 'havilah', 'hills', 'given'],\n",
       " 'famine': ['reviving', 'behave', 'bestead', 'rolled', 'bethshittah']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['god', 'lord', 'thou', 'thy', 'john', 'gospel', 'moses','famine']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
